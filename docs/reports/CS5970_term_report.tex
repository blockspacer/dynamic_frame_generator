\documentclass[11pt]{article}

% Formatting for media
\usepackage{float}
\restylefloat{table}
\restylefloat{figure}

%\usepackage[margin=1in]{geometry} 
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,array,mathtools}
\usepackage{setspace,mdframed,pgffor,color,xcolor,blindtext,cite,hyperref}
\usepackage{scrextend}
\usepackage[epsilon,tstt]{backnaur}
\usepackage{forest,tikz}
\usepackage{listings}
\usepackage[parfill]{parskip}
\usepackage{drawstack}
%\usepackage{txfonts} % uncomment this package for Times New Roman instead of Computer Modern
\usepackage[autostyle]{csquotes}
\usepackage{enumitem}
\usepackage{titling, multicol}

% Supports code formatting/highlighting
\usepackage{listings}
\lstset{language=c, basicstyle=\ttfamily, keywordstyle=\bfseries\color{blue}, stringstyle=\color{blue}, commentstyle=\color{green}, showstringspaces=false, numbers=none}
\lstdefinestyle{bash}{language=bash, basicstyle=\small\ttfamily, backgroundcolor=\color{light-gray}}
\lstdefinestyle{c}{language=C, basicstyle=\small\ttfamily,
  frame=single}
\usepackage{MnSymbol}
\lstset{prebreak=\raisebox{0ex}[0ex][0ex]
        {\ensuremath{\rhookswarrow}}}
\lstset{postbreak=\raisebox{0ex}[0ex][0ex]
        {\ensuremath{\rcurvearrowse\space}}}
\lstset{breaklines=true, breakatwhitespace=true}
\lstset{numbers=left, numberstyle=\scriptsize}
\usepackage[outputdir=output]{minted}
\usemintedstyle{autumn} % friendly, colorful
\newminted{c}{mathescape, linenos, numbersep=5pt, gobble=0, frame=lines, framesep=2mm}
\definecolor{background}{gray}{0.90}
\newminted{bash}{bgcolor=background}
\newminted{console}{bgcolor=background}
\usemintedstyle[console]{bw}
\usepackage{upquote}

\usepackage{graphicx}
%\graphicspath{{~/Dropbox/Apps/drawio/}{D:\ImagesforProjectLatex}}
\graphicspath{{images/}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\textwidth=17truecm
\textheight=20truecm
\oddsidemargin=0pt
\evensidemargin=0pt
\parindent=0pt

\def\ibf{\,\mathbf{i}\,}
\def\jbf{\,\mathbf{j}\,}
\def\kbf{\,\mathbf{k}\,}
\def\Abf{\,\mathbf{A}\,}
\def\Rbf{\,\mathbf{R}\,}
\def\Fbf{\,\mathbf{F}\,}
\def\div{\,\mathrm{div}\,}
\def\curl{\,\mathbf{curl}\,}
\def\grad{\,\mathbf{grad}\,}
\def\PP{{\mathbb P}\,}
\def\RR{{\mathbb R}\,}
\def\NN{{\mathbb N}\,}
\def\ZZ{{\mathbb Z}\,}

\usepackage[vmargin=2cm,hmargin=2cm,head=16pt,includeheadfoot]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{{\it Dynamic Frame Generation}}
\fancyhead[R]{{\it CS 5790, Spring 2019}}
\raggedcolumns

\title{\LARGE \bf
Dynamic Frame Generation Using Machine Learning\\
and Scene Data
}

\author{
Mark Wesley Harris\rule[25pt]{0pt}{0pt}\\
Sudhanshu Semwal\\
CS 5970, Wearable Computing\\
and Complex Systems
}

\begin{document}

\maketitle

\bigskip
\date{\small May 16, 2019}

%\begin{multicols}{2}
\onehalfspacing

\textit{\textbf{
\hspace{10mm} Abstract...
}}

\section{Section}
\label{ref:section_ref}
\hspace{10mm} Info

\subsection{Subsection}
\label{ref:subsection_ref}
\hspace{10mm}
One major research subarea is how to generate information from a 3D scene that is valuable to the proposed Machine Learning architecture;
I first posed the question of what data should be included.
Serializing every attribute of the scene would be excessive,
and take much to long to convert and work with.
However, using too little data would not work very well either and could produce poor results.
I realized that the data for each object must uniquely represent the objects in the scene.
Otherwise there would be too much overlapping data,
and the algorithm might never learn the differences between the input frames.
There are also no limitations nor expectations placed upon the animator;
even when comparing scenes which have the same characters, landscapes, and animations
the generated data for each object should accurately represent the properties of that object and that object alone.

\hspace{10mm}
I found through reading the implementation details of [8] that Ma et. all
used a state of the art pose recognition program to generate poses,
used as a mask to give extra input to their GAN model.
The program assigned 18 (?) key points for each pose,
which Ma et. all used as pose data in their program.
What was generated was (an image?).
These data are not mapped one-to-one with my problem, however;
what I need is for the renderer to use unrendered data as well,
to solve the problem of reflective, transparent, or other dynamic surfaces.
If the camera is facing a mirror, for example,
then the entirety of the scene to be rendered is behind the camera.
Therefore, unlike with [8] I found that using images as input would not work for this problem,
unless the image abstractly captures the scene relative to the camera.
Another obvious issue here is that we are trying to speed up rendering as much as possible;
any extra rendering could add too much time to the overall algorithm,
making it slower than rendering all frames of the animation thereby defeating the purpose of the proposed architecture.
The method in which data is stored must be fast, efficient, and encompass only a fraction of the overall runtime of the algorithm.

\section{References}
\label{ref:references}
\begin{enumerate}
\item Alexander Wilkie, Andrea Weidlich, Marcus Magnor, and Alan Chalmers. 2009. ``Predictive Rendering.'' In ACM SIGGRAPH ASIA 2009 Courses (SIGGRAPH ASIA '09). ACM, New York, NY, USA, Article 12, 428 pages.
\item R. Mahjourian, M. Wicke and A. Angelova, ``Geometry-Based Next Frame Prediction From Monocular Video''. 2017 IEEE Intelligent Vehicles Symposium (IV), Los Angeles, CA, 2017, pp. 1700-1707.
\item Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2017. ``Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge.'' IEEE Trans. Pattern Anal. Mach. Intell. 39, 4 (April 2017), 652-663.
\item Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox. ``PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes.'' CoRR abs/1711.00199 (2018): n. pag.
\item A\"{a}ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. ``Conditional Image Generation With PixelCNN Decoders.'' In Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS'16), Daniel D. Lee, Ulrike von Luxburg, Roman Garnett, Masashi Sugiyama, and Isabelle Guyon (Eds.). Curran Associates Inc., USA, 4797-4805.
\item Time Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma. 2017. ``PixelCNN++: Improving The PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications.'' ICLR Conference Paper (2017).
\item Eunvyung Park, Xufeng Han, Tamara L. Berg, Alexander C. Berg. ``Combining Multiple Sources of Knowledge In Deep CNN's For Action Recognition.'' 2016 IEEE Winter Conference on Applications of Computer Vision (WACV) (2016): 1-8.
\item Liquian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool. ``Pose Guided Person Image Generation.'' NIPS (2017).
\item Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, and Jiashi Feng. 2018. ``Multi-View Image Generation from a Single-View.'' In Proceedings of the 26th ACM international conference on Multimedia (MM '18). ACM, New York, NY, USA, 383-391.
\item Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel. ``InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.'' NIPS (2016).
\item Christoph Schied, Anton Kaplanyan, Chris Wyman, Anjul Patney, Chakravarty R. Alla Chaitanya, John Burgess, Shiqiu Liu, Carsten Dachsbacher, Aaron Lefohn, and Marco Salvi. 2017. Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination. In Proceedings of High Performance Graphics (HPG '17). ACM, New York, NY, USA, Article 2, 12 pages.
\end{enumerate}
%==========================================================
%\end{multicols}
\end{document}
%==========================================================
