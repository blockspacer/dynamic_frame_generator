\documentclass[conference]{IEEEtran}
\usepackage[english]{babel}
\usepackage{cite,setspace}
\usepackage[autostyle]{csquotes}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Dynamic Frame Generation Using Machine Learning
and Scene Data
}

\author{\IEEEauthorblockN{Mark Wesley Harris}
Dr. Sudhanshu Semwal, CS 5790\\
University of Colorado\\
at Colorado Springs\\
May 16, 2019}

\maketitle

\begin{abstract}
This paper describes my preliminary research and implementation of a
system for dynamically generating frames or portions of frame
of a given animation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Rendering the frames of a production-level quality animation can take an
excessive amount of time. As stated in \cite{ref:monsters}, ``All told, it has
taken more than 100 million CPU hours to render [Monster's University] in its final form. If
you used one CPU to do the job, it would have taken about 10,000 years to
finish. With [the supercomputer]\dots it took a couple of years to
render.'' The slowness of high quality rendering is all the more pertinent when considering all of the
pre-film preduction, such as intermediate renderings, changes to the scene, and
recasting or changes in the script. If there was a way to speed up the rendering
process, it would greatly benefit feature film animation and 3D effects companies such as Pixar
and Industrial Light \& Magic.

So how might we be able to speed up rendering? The main bottleneck that
researchers of this topic have been struggling with is the level of detail
encapsulated within a single frame of an animation. For example of this, Figure
\ref{fig:incredibles} shows a close up of of Bob Parr's shirt from the trailer
of ``Incredibles 2'' (released by Pixar June 15, 2018). There is enough
resolution on that image to clearly show not only the detailed stitching of the
fabric, but also strands of thread extruding from the shirt itself. While a
viewer most likely won't be able to discern these details during the running
movie, the quality tricks their subconscious into believing they are
experiencing something real; these details help convey more emotion and
connections to the world inhabited by the characters on the screen.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{incredibles.png}}
\caption{Level of detail in Pixar's ``Incredibles 2''.}
\label{fig:incredibles}
\end{figure}

\subsection{Problem Statement}
\label{subsec:problem_statement}
So if this bottleneck cannot be surpassed by lowering the quality, perhaps we
can overcome it through rendering less frames. This is the foundationary concept
of my project. The production pipeline would work as follows:

\begin{enumerate}
\item Artists create content as they would normally.
\item When the animation is ready to be rendered as a video, only every other
frame is rendered.
\item These frames, are then passed into a preprocessing stage to generate
what I call
``frameblocks'' (see Section \ref{subsec:frameblocks} for more information).
\item Scene data for each frameblock is extracted.
\item All collected frameblock/scene data pairs are passed into a generator/discriminator
Machine Learning architecture (see Figure \ref{fig:block_diagram}) to train an algorithm
to recognize the relationships between the rendered frame and scene data.
\item Once the algorithm is sufficiently trained, scene data for the missing
frames is input into the generator system in order to create frameblocks
representing the previously unseen rendered frame.
\item These generated frameblocks are stitched together (perhaps using another Machine
Learning model) in order to output a final rendered frame.
\end{enumerate}

There are many benefits to using this system. The most obvious is the ability
for artists to select the frames that are input into the algorithm; using half
of an animation's frames guarantees most changes will be captured in the
generated frameblocks, however artists may also include frames that have
uniqueness they would want to be recognized by the algorithm. Another benefit is
the ability to apply the trained model to similar 3D scenes, or to small changes
of the trained 3D scene. Thus the animation pipeline itself could be sped up
significantly by rendering predictions of how an animated scene will look in
realtime.

My hypothesis is that using the system outlined in this paper, rendering the
final animation will take less time than it does currently and animators will
have more flexibility for predicting what their animations will look like once
rendered. For this paper I first in Section \ref{sec:related_concepts} work
related to my project, then provide an overview
of my proposed architecture in \ref{sec:architecture}. Section
\ref{sec:deliverables} presents what I focused on for this term, and finally
Section \ref{sec:implementation} I then discuss what I implemented.
I conclude with Section \ref{sec:future_work} to discuss my plans for the future of
the project.

\section{Related Concepts}
\label{sec:related_concepts}
I found found many research concepts, especially those relating to Machine Learning,
which provide the foundations for implementating this project.
In Section \ref{subsec:applicable} I discuss the
research (much of it cutting-edge) which has provided me insight into the
components that make up the final architecture I will implement and evalulate.
Then in Section \ref{subsec:complex_systems} I
touch on concepts from Complex Systems to discuss the relationships between
the 3D animated scene and dynamism, and the complexities of what needs to be
recognized by the Machine Learning models.

\subsection{Applicable research}
\label{subsec:applicable}
I have found many papers that are applicable to my stated problem. I would like
to discuss 4 of them in detail as well as provide
discussion on how they could be applied to my project implementation.

\cite{ref:frame_prediction} (``Geometry-Based Next Frame Prediction From
Monocular Video'') focuses on generating the
next frame of a monocular (i.e. single-view) video 
given the video's previous frames. The researchers deployed
an LSTM Machine Learning model in order to generate a depth map representing the next frame.
Another Machine Learning model was then applied to convert the generated depth
map into the pixels of the predicted frame. The largest deficiency of their
project was the poorness of the output frame's quality as shown in Figure
\ref{fig:frame_prediction}; in order to use their
application for production-quality outputs, more steps would have
to be taken to ensure the quality is comparable to the quality of the original input frames.
Concerning applications to my
project, what was most note-worthy is the use of the LSTM model in order to
collect changes from all previous frames instead of just one previous frame;
I feel this type of mechanism will be
crucial for the generative model described in Section \ref{sec:architecture}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{frame_prediction.png}}
\caption{Example of output frame and depth map of \cite{ref:frame_prediction}.}
\label{fig:frame_prediction}
\end{figure}

\cite{ref:spatiotemporal} (``Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for
Path-Traced Global Illumination'') concentrates on
generating high quality outputs given very low quality input
frames. The inputs to the algorithm are rendered frames using one path-per-pixel global
illumination. As shown in Figure \ref{fig:spatiotemporal}, they are able to
transform very crude, noisy frames into production-grade outputs comparable to
the frames rendered at full resolution. Their algorithm is essentially a
multi-pass filter which the inputs are fed into, making it very fast and
reliable. My project benefit's from the foundational algorithms presented in
this research, however the overall premises are very different; my problem
requires frame generation given scene data that has not been rendered at all,
while theirs depends on entire rendered frames.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{spatiotemporal.png}}
\caption{Example input and output frame of \cite{ref:spatiotemporal}.}
\label{fig:spatiotemporal}
\end{figure}

\cite{ref:posecnn} (``PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in
Cluttered Scenes'', 2018) proposes a framework called ``PoseCNN'' which
employes a CNN to estimate pose information and object attributes given an input
image. The general architecture is shown in Figure \ref{fig:posecnn}; an image
is input into the system, and what is output are semantic labels, 3D
translation, and 3D rotation of each object. The researchers deem the collection
of these outputs ``6D'' data, since there are 6 dimensions total. This
research is very applicable to my project, since I need to generate some form of
the same information about objects in a 3D scene to use as inputs into my
system. I will consider the author's references and reasoning when making those
decision for the scene data inputs of my project.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{posecnn.png}}
\caption{Example architecture for \cite{ref:posecnn}.}
\label{fig:posecnn}
\end{figure}

\cite{ref:pose_guided} (``Pose Guided Person Image Generation'', 2017)
presents a system entitled
``Pose Guided Person Generation Network'' (abbreviated as
PG\textsuperscript{2}). This system takes as input a condition image
of a person in a given pose, and an image representing the targe pose the person
should be translated into (the choice of the representation for pose data is explained further in the
paper). Using two difference generators,
the system produces as intermediate output a low-resolution generated
image that defines global structure and features for the new pose, as well as a final refined
image of the person in the new pose (see Figure \ref{fig:pose_guided} for
details on their project architecture). I found this research directly applicable
to my project, since both problems involve combining an image with extraneous data. There are, however,
some differences between my project and theirs. As explained more in their
paper, the authors were able to represent pose data as a standardized black and white image.
Since my project requires the use of scene data outside of the rendered
boundaries (i.e. what the camera can ``see''), I would find it difficult to
obtain good results with the same approach. As I also found with the research in
\cite{ref:spatiotemporal}, this project requires an input image while the only
input of my final generative architecture is scene data itself. I will need to
keep this in mind during my investigation of what Machine Learning models to
develop for the generator. Despite these counterpoints, I do find the overall
system achitecture to most closely represent my target architecture; I plan to
have two generators and one discriminator, which can be trained on frame and
scene inputs.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{pose_guided.png}}
\caption{System architecture proposed by \cite{ref:pose_guided}.}
\label{fig:pose_guided}
\end{figure}

\subsection{Concepts from Complex Systems}
\label{subsec:complex_systems}
My interest in the field of computer animation is what sparked my desire to
tackling one of the hardest problems studied therein; that is, given any 3D scene
predict what will rendered to the screen. The problem does not sound too
difficult, since the types of data for the scene is finite; however if we
consider the infinite number of possible interactions within the scene itself,
the problem space grows exponentially.

In order to facilitate discussion on how the field of Complex Systems relates to
my project, I would first like to provide a definition of a ``complex system''
based what we studied in class:

\blockquote{
A complex system is a system made up of small components interacting with each
other to produce emmergence and to create global phenomena without a centeralized
controller. The system must also adapt to context and environment, meaning it is
impossible to predict the state it is in even if we were to know all laws
affecting the system and all initial starting conditions; the system is
nonlinear by nature of its complexity.
}

I believe my project abstractly encapsulates all the qualities of a complex system;
we are not concerned with the state of the animation software, but rather the
complexity of predicting the pixels that make up a rendered frame of an
animation. What is displayed by these pixels is completely dependent upon how
the objects in the scene interact with one another, however at times it is difficult
to find these interactions.
Anomalies such as these create the emmergent behaviour that makes 3D animation so rich
an artform.

What might we consider as the ``global phenomena'' for a 3D scene?
To answer this question, we must examine the various parts that make up a scene.
We are only concerned with components that impact the visual rendered by the
camera, and so components such as scripts, heirarchy data, etc. can be safely
excluded from consideration. Any physical objects, including meshes, particle
systems, lights, and cameras, contain valuable information pertinent to
what will be rendered; material attributes (e.g. opacity, reflectivity, or emission),
physical characteristics (e.g. size, shape, or behaviour), and animated movement can all
impact the rendered pixels for other objects in the scene. Essentially, what is rendered to the
screen is a combination of all of these characteristics for each object.
This behaviour increases the visual richness of what is rendered, however in the
same way also increases the complexity of defining how objects interact with
each other. After also considering the limitless possibilities for the objects
in a 3D scene,
it becomes clear that no iterative algorithm will ever be good enough
to provide a solution to this problem. Therefore this problem is
nonlinear and requires an abstract nonlinear approach in order to be solved.

\section{Architectural Overview}
\label{sec:architecture}
The architecture I outline here is based off of the research presented in
\cite{ref:pose_guided}. As I mentioned before, this architecture has several
components which I believe are important to my system. Below I outline the three
most important components which I plan to include in my architecture (see Figure
\ref{fig:block_diagram} for a block diagram of my architecture):

\begin{enumerate}
\item Generator I: uses a Convolutional Nueral Network (CNN) model to generate a
low resolution image containing global structures found in the
source data. This generator will likely be the most important component to
research, since I have not found a previously developed model to best suit the
generator's purposes. To accomplish my desired end result,
I believe I will need to combine the research of the LSTM
model from \cite{ref:frame_prediction} with the
generative CNN models from \cite{ref:pixelcnn_decoders}, \cite{ref:pixelcnn++},
and \cite{ref:multi_source}, and the research of semantic data from
\cite{ref:image_captioning} and \cite{ref:posecnn}. Because of the apparent
level of developmenet necessary for this compoenent, I have
outlined it in orange as shown in Figure \ref{fig:block_diagram}.
\item Generator II: uses a fully connected CNN with the output of the first generative network and a
condition image to generate a detailed output suitable as pixels of a rendered
frame. I believe this network will be easier to construct than the model for the
first generator, since it is likely I will be able to use roughly the same approach as
\cite{ref:pose_guided} did for their second generative network. One difference I
predict is the necessity for training data spanning multiple frames, which again
could touch on the LSTM model presented in \cite{ref:frame_prediction}.
\item Discriminator: trains a network to recognize a fake image from a real
image, thereby driving both generators to do a better job of creating their
respective outputs. This component is very well-researched by current standards,
and I should have little difficulty replicating the work of
\cite{ref:pose_guided} in order to create a working discriminator.
\end{enumerate}

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{block_diagram.png}}
\caption{Generalized project block diagram.}
\label{fig:block_diagram}
\end{figure}

\section{Project Deliverables}
\label{sec:deliverables}
For my project this term, I focused on creating the inputs of my target
architecture. In the following subsections, I ellaborate
on the reasoning for how these inputs are difined. This work is essential to my
proposed system, since the generator/discriminator model is useless if input
training data is not representative what what we'd like to be generated.

\subsection{A discussion on system inputs}
\label{subsec:inputs}
A major subarea of research for this project is how to generate the
information from a 3D scene that would be valuable to the proposed architecture
(see Figure \ref{fig:block_diagram}).
When first researching this topic, I posed the question of what
data should be sent to Generator I.
In there research, \cite{ref:pose_guided} implemented a similar schema where the input to this
component of their system was a condition image and target pose.
Ma et. all uses a state of the art pose recognition program to
generate poses, and used this as a mask to give extra input to their generative model.
The program assigned 18 ``keypoints'' for each pose,
which were used as the pose representations for each iteration.
Therefore the pose data was represented as a black and white image,
as shown in Figure \ref{fig:pose_guided}.
Disregarding the architectural changes to the generator, I predict I too will
have similar inputs; one or more pairs of frame images to scene data.

These data are not mapped one-to-one with my problem, however;
what I need is for the renderer to use unrendered data as well,
to solve the problem of reflective, transparent, or other dynamic surfaces.
If the camera is facing a mirror, for example,
then the entirety of the scene to be rendered is behind the camera.
Therefore, unlike with \cite{ref:pose_guided}
I found that using images as input would not work well for this problem,
unless the image abstractly captures the scene data relative to the camera and
the objects within the rendered view.

However, serializing every attribute of the scene for each frame would take too much time
and result in a bloated file size.
Using too little data could also produce poor results when training the
generator. Thus there are some requirements for this extra data:
\begin{enumerate}
\item Data must be small in size but representative of the objects which it is
describing. If any characteristics of the scene are left out of this
description, the trained algorithm may produce incorrect results in practice.
\item Data must also uniquely represent the objects in the scene and how they
interact with their environment.
If there is an abundance of similarities between objects, then
the algorithm might never learn the differences between the input frames.
\item There are also no limitations nor expectations placed upon the animator;
even when comparing scenes which have the same characters, landscapes, and
animations, the generated data for each object should accurately represent
the properties of that object alone. If all the objects in the scene are the
same, then the focus would be on how they interact with each other in the scene
during animation.
\end{enumerate}
Another obvious issue here is that we are trying to speed up rendering as much as possible;
any extra rendering could add too much time to the overall algorithm,
making it slower than rendering all frames of the animation
and thereby defeating the purpose of using the proposed architecture.
The method in which data is stored must be fast, efficient, and encompass only a fraction of the overall runtime of the algorithm.

\subsection{Frameblocks}
\label{subsec:frameblocks}
There are many different ways to choose the inputs for the program.
I believe the problem does not involve only the color values of each
rendered pixel,
but rather the relationship between that pixel and the scene data which
generated it. Thus, not all pixels should be trained on, especially if they have
little to do with how the rendered scene is changing.
It is clear a preparation phase is necessary for all frames of a given
animation; portions of each frame will become the inputs for the larger
architecture proposed (see Figure \ref{fig:block_diagram}).
I have defined these inputs as ``frameblocks''.
Frameblocks represent a small ($\approx 3\%$) portion of a given frame,
and it is assumed
that a frameblock is representative of the changes between
two frames that are significant enough to
be used as input.
This concept of frameblocks also satisfies the
requirements enumerated in Section \ref{subsec:inputs}.

Thus, the selection of frameblocks depends upon how much the scene changes
between frames and in what ways;
just because a frame is keyed does not imply there was any major change.
As with complex particle systems, simply existing in the scene is all that is
needed to create great changes in the rendered output;
this is also why it is so pertinent that the later stages of the arichtecture
take into account both scene data and rendered pixels.
Therefore we cannot rely on scene data in our selection of frameblocks, rather
this selection must come from the changes in the frames themselves.
This problem is not data-specific,
however, as is the case with the later Machine Learning models.
If the pixel data were converted into a format which showed changes and hid similarities,
then it could be possible to train a general algorithm which could be queried
for any given scene data input.

%They are defined below (frames are indexed by $f_i$ for $i = 1,2,...,n$ where $n$ is the number
%of frames to train on):
%\begin{enumerate}
%\item The first frame, $f_1$, is automatically queried
%since it provides a baseline for changes shown in subsequent frames.
%\item The rest of the frames are processed as follows:
%\begin{enumerate}
%\item Frame $f_i$ 
%\end{enumerate}
%\item For each subsequent frame $f{i=2...n-1}$, the respective pixel values for
%frame $f_i$ and $f_{i - 1}$ are XOR'ed, and these results are summed together
%(perhaps the summation could be capped at 255, in order to save memory).
%\end{enumerate}
%This new composite image contains data on what has changed between frames,
%and it is to be evaluated by the Phase 1 Machine Learning algorithm to determine if the frame has enough changed values.
%But we should ask ourselves, is a Machine Learning algorithm necessary at this point?
%Surely we could just use the ratio of dark pixels to light pixels to make this decision.
%As stated before we are not only concerned with how much the frame changed overall but how it changed and where it changed.
%There is a problem with using frames that change too much, especially if the frames could be classified as separate scenes;
%each scene must be processed separately.
%Thus the decision of whether or not to use a given frame block for the current scene should be a trained decision,
%where the training data comes from similar analysis of movie frames where the scenes and changing areas are clearly defined.

\subsection{Buffer Frameblocks}
\label{subsec:buffer_frameblocks}
Concerning frameblocks, an important question still remains:
how do we capture changes not significant enough to trigger a frameblock as
described in sections \ref{subsec:inputs} and \ref{subsec:frameblocks}?
Derp...

Even if we captured all blocks that changed, how many of those changes would be similar?
Depending on the frame rate and how many blocks changed,
it could mean that most of the same blocks had similar changes which don't need to be retrained by the program.
Given that a scene could have several seconds, if not minutes, of correlated data at a time,
it is only natural to crop out important portions of the scene to use for training.
Therefore there must also be a dynamic mechanism to select which frames classify as part of the current ?scene? and of those frames which to use as inputs to train on.


\subsection{Training The Algorithm}
\label{subsec:training_the_algorithm}
What of training the algorithm? What kinds of datasets should be used?
How many data should be included in each dataset for effective training to occur?
Usually, for a Deep Learning project, small images on the order of 10's of thousands are necessary to create a well-trained algorithm.
Where might the training data originate from my project, however?
The obvious choice is that it must be a subset of rendered frame data.
These frames, used in combination with corresponding scene data,
would be used as Ground Truth to train a GAN on how to produce images given scene and random inputs.
As described in \cite{ref:pose}, these types of inputs can be used to create low-resolution images,
which are then used as the random data for the second GAN which is responsible for creating high-resolution images.
For my research there are also CNN LSTM qualities to consider,
since all frames for a given ?scene? are related.
Multiple frames could be used as inputs,
therefore using frames more than once and getting more use out of the training dataset.
However, if the training data originates from the current scene,
would there be enough frames to justify the use of such a system?
If there is only a single frame rendered previously to be used as training data, the answer is simple;
render the other frame from scratch! This is obviously too little data to support wholesome Machine Learning;
then, we must ask, where is the cut-off? What variables could this depend on?
Size of the scene? Number of keyframes? Number of vertices, edges, or faces?
Materials, textures, and attributes? Oh but surely,
couldn't we simply say the complexity of the scene as a whole?
Therein lies the problem, for we must first classify what complexity means;
there must be some ruler by which to measure the complexity of a scene in terms of tangible data for Machine Learning.

So, given there are enough rendered frames respective to the current scene,
how might we use them in training? CNN's employ the powerful technique of (as one might guess) convolution.
They are able to extract data from the whole of an image without extra identification.
Could a similar technique be applied here? Most certainly, given any small portion of an animated scene,
is there anyone who could say where it came from on the screen?
Without viewing other portions of the scene as well, comparing their composition,
and piecing together the rest of the jigsaw puzzle, there is no way to determine from where on the screen a portion originates.
This is by nature of the 3D world; no object has simply one view and one view only,
the rotation, position, and scale of any object depends on the camera view itself.
Therefore, I feel it very possible to split a single frame into several pieces,
each with its own ways of seeing the 3D world around them.
Especially with extremely high resolutions; a 32x32 pixel block may take up half of one resolutions,
while merely a hundredth of another. It is the higher end of this spectrum for which I wish to apply my algorithm,
and so I will assume that the blocks of each frame will be a hundredth if not less of each frame.
And what of frame rate in producing the quantity of rendered frames?
Framerate does indeed play a large role in how many frames are produced,
given how many seconds long the clip of an animation is.
60 fps or higher is quite common for high quality gaming applications,
however these would not give enough data to support the proposed
Machine Learning methodology since future frames and training time is necessary.
Real time applications cannot be assisted through this means.
Therefore, we must focus on the standard for animation frame rates.
As stated in [], studio standards currently average to * fps (assuming 30),
which is roughly half that needed in real time rendering.

Given these new attributes of the dataset,
how then do the quantity and quality of renders affect training capabilities?
Well we must ask several questions.
How high of a resolution must the frame be, how many blocks can exist per frame,
and how many frames are there per second;
only then can we finally stipulate how many seconds is to be expected by this program.
Let's assume that 32x32 pixel blocks is sufficient, and that a frame is roughly 1600x1600 pixels (fairly high resolution).
This leaves us with 50x50 blocks per frame, or 2500 created blocks total.
Similarly, if we assume there are 30 frames per second and half are rendered to be used as part of the training dataset,
that leaves us with 37,500 images per second (or 37.5 per millisecond).
Now we can finally conjecture as to how many seconds of rendered data would be worth using this approach,
and it appears that 1 second might very well be enough!
However there is one more factor which we have not yet considered; how much changes within fractions of a second.
In order to be considered as part of the same scene, and similarly have enough merit to train over,
an object would have to change very minutely each millisecond.
There are certain things which behave according to this logic, such as the movement of living things.
However, there are also objects such as particle systems which are intended to give the impression of randomness by changing very rapidly.
These two types of motion cannot be classified the same way;
it is very likely they will need to be dealt with dynamically!
For instance, one might imagine the animation of characters talking inside of a cafe on a rainy day.
The frameblocks that render the rain through the window are changing very rapidly,
while those that contain the characters are most likely changing very slowly.
So why not accommodate this phenomenon by quing frameblocks and their scene configuration data based upon the block itself;
unchanging blocks need not be queued for further processing,
as this could lead to overfitting of the unchanging blocks and underfitting of the dynamic blocks.

\subsection{Training Analysis}
\label{subsec:training_analysis}
However clustering is a problem better suited for data with multiple unique qualities,
much like a problem for n-dimensional space.
This data does not represent anything more than the amount of difference between two frame's pixels.
The argument can be made that there may be waste in the frameblocks; take for example if a
frame block edge straddles an area that just barely meets the criteria for processing while neither frame block will be processed.
So a set configuration is ineffective for selecting frameblocks, since there may be missing frames in the selection.
What about a sliding window? But then by how much should this window slide by? And what if there is too much overlap between frameblocks selected?
Well if there is too much overlap, too many of the same frame will be selected and the algorithm can overtrain.
If there is too little coverage, then frames will be missing.
The solution? Perhaps a window that slides by half the width of a frame block, and half the height of a frame block.
Should there be any extra rules to decide if a block indeed qualifies?
For example, if two blocks are chosen because they have the exact same changes shown in both, should one of them be discarded?
And should frameblocks be optimized to show changes in the center instead of on one side?
Perhaps the frame should be dynamically chosen based upon threshold, shifted by 1 pixel each time until the frame has enough changes shown.
A summation of all pixel values of the frame block could be applicable,
and once a frame is found the offset could be set to half the frame block width
(and height, when next changing rows).

I also feel that the changes should not be a set value either; perhaps it should be a percentage of the total change overall.
Take for example a slowly changing gradient from one color to another (changing over multiple frames).
We would like to include this, since it is an important lerp to show in the generated frame,
however the changes would be continuous over the entire frame. This is a big problem!
How do we manage to only capture the frames we need for each configuration?
Perhaps previous frameblocks should be captured and hashed, so that similar frameblocks are not captured.
To do this I propose we take the first 5 bits of each color value for hashing (so that the last 8 possible values are ignored).
Thus there are 15 bits to hash per pixel, and 32 x 32 pixels. I should research further what type of hash could be done.
Some data loss is ok, since we want to disregard frameblocks that are too similar.
Then maybe values should be super-sampled (averaged?) in smaller blocks to create a smaller 15 bits x 8 x 8 pixel image to compare against.
However how much similarity is acceptable? There are probably too many ways to create the generated 8 x 8 pixel image based off of the given frames.
Either way, it is clear that some method of hash could be used to avoid training on repeated blocks.

In summary, the total process would be 1) find the frame block (by so far an unknown method), 2) check frame against previously found frames, 3) generate frame block and add to hash list. When the frame is completely processed, clear the list of hashed frameblocks.

\section{Implementation}
\label{sec:implementation}
Derp intro.

\subsection{Frame Processing}
\label{subsec:frame_processing}
The process for generating frameblocks takes in an input of 2 images and
outputs all frameblocks which meet our image processing requirements.
Given these two image inputs,
we must decide what portions of them are different enough to be used as frameblocks.

For this problem, we will need to process every frame with its neighbors.
Luckily, the left neighbor will have already been calculated in the previous step.
So we only need to compare each frame to its right neighbor for frameblock generation.

Now we must read in the frames and process them.
Boundaries are also chosen here, in case the images are different sizes.
In theory for this wouldn't occur since the frames of an animation are always the same dimensions,
however it doesn't take much time to check. Below the two images are plotted side by side,
as a reminder of how different they appear.
In theory these images should be two consecutive frames,
which usually means they will have very small differences.

\subsection{Shadow Image Creation}
\label{subsec:shadow_image_creation}
Here the pixels of each image are processed.
A summation of pixel values is recorded while the loop is running;
each pixel adds it's red, green, and blue (RGB) values together,
caps the value at 255, and then adds this to the pixel\_sum variable.

My goal is to find pixel $P(x,y)$,
which represents the difference between the two input
pixels at the coordinates $(x,y)$ for each image.
I call the image created out of the resulting pixels the "shadow image" for the two inputs.
For our purposes, let $p_i(x,y)$ represent the pixel at coordinate
$(x,y)$ for image $i$ and let $\oplus$ represent the XOR operation.
Thus for each pixel of the input images we calculate the following value:

$$P(x,y) = p_1(x,y) \oplus p_2(x,y)$$

and store this as the value for the output image.
Below is the calculated output image.
This image graphically shows the differences between frames,
where white represents no difference and black represents the maximal difference.
We are able to use this later to calculate frameblocks.

\subsection{Frameblock Selection}
\label{subsec:frameblock_selection}
We select frameblocks using the above calculated image.
Frameblocks are selected based on the percentage of black pixels
(which in this case represents maximal difference).
To calculate the percentage of black pixels,
the total sum of pixel values in the shadow image is divided by how many black pixels are possible in the image.
Thus the value represents the probability of encountering a changed pixel between the two input images.
If the ratio calculated in this way is very low,
that means there are very few black pixels so we need to lower our standard for selecting frameblocks.
Similarly, if the ratio is close to 1,
that means there are many more black pixels and we should raise the standard for selecting frameblocks.
This value is directly tied to the input frames.

We need a sliding window to process frameblocks,
since we want to skip as many pixels of the same image as possible.
The overlap for each frameblock horizontally and vertically is given from the
block\_offset variable set in the beginning of the program.
A value of 1 means no overlap is possible,
a value of 2 gives 50\% possible overlap, and so on.

Each frameblock selected is saved to an image file.
A red rectangle is also overlaid on top of the original calculated image to represent where each frameblock was selected from.
Frameblocks which are found to not show enough changes to be included in this iteration are used to find differences spanning multiple frames (described later).

\subsection{Buffer Frameblocks}
\label{subsec:buffer_frameblocks}
We stored the frameblocks that failed to pass the test this iteration in two formats:
an inverted shadow image file and a frame image file.
The shadow image file contains the calculated pixel\_sum for each pixel of the ROI of the original shadow image,
added together with any previous buffered frameblocks.
Thus each frameblock is not only a comparison between the current and next frames,
but instead a comparison of all the frames which didn't pass before.
It is clear to see from this that even if the change is gradual,
it will eventually be represented in the training data.
The ROI from the first input image is also saved so that this change can be accurately captured.
If the frameblock never changes enough over the entire course of frameblock generation,
it is not included in the dataset.
An example pair of inverted shadow and frame buffer images is shown below.

\subsection{Data Analysis}
\label{subsec:data_analysis}
So, we were able to generate frameblocks, but how are we to know they are indeed all correct?
I propose we compare these outputs to a different scene.
As it so happens, the scene processed above is much more complex than we might have realized before.
Each sphere is transparent and very reflective,
so the changes to one sphere impact changes to all the spheres in the scene.
The process used above is repeated below, but for spheres that are completely opaque and diffuse;
thus the algorithm is able to filter out more data and choose only frameblocks which change between the two frames.

\subsection{Conclusions}
\label{subsec:conclusions}
The process shown here is an effective means of choosing frameblocks
for training the proposed Machine Learning algorithm.
By carefully deciding which portions of the frame to use,
we guarantee there will be minimal over-training data and maximum changes shown in the dataset.
The next steps of this process will be to develop the Machine Learning algorithm itself,
which will take as input the frameblocks of every other frame in an animation.

We have solved the problem of what input would be appropriate to train
the proposed Machine Learning algorithm;
the dataset generated should contain enough images with enough differences between them.
The other input (i.e. the scene data) has not been decided yet.
Deciding what aspects of the scene to bundle with each frameblock will require more research and experimentation.

\section{Introduction}

\section{Ease of Use}

\subsection{Maintaining the Integrity of the Specifications}

%\begin{table}[htbp]
%\caption{Table Type Styles}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%\cline{2-4} 
%\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%\hline
%copy& More table copy$^{\mathrm{a}}$& &  \\
%\hline
%\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%\end{tabular}
%\label{tab1}
%\end{center}
%\end{table}

%\begin{figure}[htbp]
%\centerline{\includegraphics{fig1.png}}
%\caption{Example of a figure caption.}
%\label{fig}
%\end{figure}

\begin{thebibliography}{12}
\bibitem{ref:bessel_functions} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{ref:predictive_rendering} Alexander Wilkie, Andrea Weidlich, Marcus Magnor, and Alan Chalmers. 2009. ``Predictive Rendering.'' In ACM SIGGRAPH ASIA 2009 Courses (SIGGRAPH ASIA '09). ACM, New York, NY, USA, Article 12, 428 pages.
\bibitem{ref:frame_prediction} R. Mahjourian, M. Wicke and A. Angelova, ``Geometry-Based Next Frame Prediction From Monocular Video''. 2017 IEEE Intelligent Vehicles Symposium (IV), Los Angeles, CA, 2017, pp. 1700-1707.
\bibitem{ref:image_captioning} Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2017. ``Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge.'' IEEE Trans. Pattern Anal. Mach. Intell. 39, 4 (April 2017), 652-663.
\bibitem{ref:posecnn} Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox. ``PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes.'' CoRR abs/1711.00199 (2018): n. pag.
\bibitem{ref:pixelcnn_decoders} A\"{a}ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. ``Conditional Image Generation With PixelCNN Decoders.'' In Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS'16), Daniel D. Lee, Ulrike von Luxburg, Roman Garnett, Masashi Sugiyama, and Isabelle Guyon (Eds.). Curran Associates Inc., USA, 4797-4805.
\bibitem{ref:pixelcnn++} Time Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma. 2017. ``PixelCNN++: Improving The PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications.'' ICLR Conference Paper (2017).
\bibitem{ref:multi_source} Eunvyung Park, Xufeng Han, Tamara L. Berg, Alexander C. Berg. ``Combining Multiple Sources of Knowledge In Deep CNN's For Action Recognition.'' 2016 IEEE Winter Conference on Applications of Computer Vision (WACV) (2016): 1-8.
\bibitem{ref:pose_guided} Liquian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool. ``Pose Guided Person Image Generation.'' NIPS (2017).
\bibitem{ref:multi_view} Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, and Jiashi Feng. 2018. ``Multi-View Image Generation from a Single-View.'' In Proceedings of the 26th ACM international conference on Multimedia (MM '18). ACM, New York, NY, USA, 383-391.
\bibitem{ref:info_gan} Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel. ``InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.'' NIPS (2016).
\bibitem{ref:spatiotemporal} Christoph Schied, Anton Kaplanyan, Chris Wyman, Anjul Patney, Chakravarty R. Alla Chaitanya, John Burgess, Shiqiu Liu, Carsten Dachsbacher, Aaron Lefohn, and Marco Salvi. 2017. Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination. In Proceedings of High Performance Graphics (HPG '17). ACM, New York, NY, USA, Article 2, 12 pages.
\bibitem{ref:monsters} Dean Takahashi, ``How Pixar made Monsters University, its
latest technological marvel''. April 24, 2013. Article.
\end{thebibliography}
%==========================================================
\end{document}
%==========================================================
